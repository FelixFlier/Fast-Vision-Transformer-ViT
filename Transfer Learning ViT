""" 
Transfer Learning ViT:
Uses a small pre-trained ViT base model
Implements transfer learning with layer freezing
Uses augmentation
"""

import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import timm
from torch.optim import AdamW
import matplotlib.pyplot as plt

class FastTransferViT(nn.Module):
    def __init__(self, num_classes=10):
        super(FastTransferViT, self).__init__()
        
        # Use smaller pretrained model
        self.vit = timm.create_model(
            'vit_tiny_patch16_224',
            pretrained=True,
            num_classes=0
        )
        
        # Simplified classification head
        self.classification_head = nn.Sequential(
            nn.Linear(192, 128),  # 192 is tiny ViT's feature dim
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, num_classes)
        )
        
        # Freeze early layers
        self.freeze_layers()
    
    def freeze_layers(self):
        # Freeze first 4 blocks (half of the transformer blocks)
        for name, param in self.vit.named_parameters():
            if 'blocks' in name:
                block_num = int(name.split('.')[1])
                if block_num < 4:
                    param.requires_grad = False
    
    def forward(self, x):
        features = self.vit(x)
        return self.classification_head(features)

def train_model(model, train_loader, val_loader, num_epochs=5):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    model = model.to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)
    
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        
        for i, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # Simple gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            running_loss += loss.item()
            
            if i % 20 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}')
        
        # Validation
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        print(f'Epoch [{epoch+1}/{num_epochs}]')
        print(f'Training Loss: {running_loss/len(train_loader):.4f}')
        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')
        print(f'Accuracy: {100.*correct/total:.2f}%')
        print('-' * 50)

def main():
    # Simple augmentation
    train_transform = transforms.Compose([
        transforms.Resize(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485], std=[0.229])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485], std=[0.229])
    ])
    
    # Load and subset data
    train_dataset = datasets.CIFAR10(root='./data', train=True,
                                   download=True, transform=train_transform)
    test_dataset = datasets.CIFAR10(root='./data', train=False,
                                  download=True, transform=val_transform)
    
    # Use 20% of the data
    train_dataset = Subset(train_dataset, range(0, len(train_dataset), 5))
    test_dataset = Subset(test_dataset, range(0, len(test_dataset), 5))
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(test_dataset, batch_size=32)
    
    model = FastTransferViT(num_classes=10)
    train_model(model, train_loader, val_loader)
    
    # Save model
    torch.save(model.state_dict(), 'fast_transfer_vit.pth')

if __name__ == "__main__":
    main()
