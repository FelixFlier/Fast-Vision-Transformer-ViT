"""
Uses the smallest ViT model (vit_tiny)
Reduced dataset size
Faster training cycle
uses different Modifications
"""

import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import timm
from torch.optim import Adam
import matplotlib.pyplot as plt

class FastViT(nn.Module):
    def __init__(self, num_classes=10):
        super(FastViT, self).__init__()
        
        # Modification 1: Using the smallest available ViT model
        # vit_tiny is much faster than larger variants
        self.vit = timm.create_model(
            'vit_tiny_patch16_224',
            pretrained=True,
            num_classes=num_classes
        )
        
        # Modification 2: Simple feature projection layer
        # Added before the ViT to reduce input complexity
        self.feature_projection = nn.Sequential(
            nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1),
            nn.ReLU()
        )

    def forward(self, x):
        # Apply feature projection
        x = self.feature_projection(x)
        # Pass through ViT
        return self.vit(x)

def train_model(model, train_loader, val_loader, num_epochs=3):  # Reduced epochs
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr=0.001)
    
    # Lists to store metrics
    train_losses = []
    val_accuracies = []
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        
        # Use enumerate to show progress
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            
            # Print progress every 50 batches
            if batch_idx % 50 == 0:
                print(f'Epoch {epoch+1}, Batch {batch_idx}: Loss = {loss.item():.4f}')
        
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Quick validation
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        val_accuracies.append(accuracy)
        
        print(f'Epoch {epoch+1}:')
        print(f'Training Loss: {avg_train_loss:.4f}')
        print(f'Validation Accuracy: {accuracy:.2f}%')
        print('-' * 30)
    
    return train_losses, val_accuracies

def main():
    # Simplified data transformations
    transform = transforms.Compose([
        transforms.Resize(224),  # ViT requires 224x224 input
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])  # Simple normalization
    ])
    
    # Load only a subset of CIFAR-10 for faster training
    train_dataset = datasets.CIFAR10(
        root='./data', 
        train=True, 
        download=True, 
        transform=transform
    )
    test_dataset = datasets.CIFAR10(
        root='./data', 
        train=False, 
        download=True, 
        transform=transform
    )
    
    # Use smaller subsets of the data
    train_dataset = torch.utils.data.Subset(
        train_dataset, 
        indices=range(0, len(train_dataset), 5)  # Use 1/5 of training data
    )
    test_dataset = torch.utils.data.Subset(
        test_dataset, 
        indices=range(0, len(test_dataset), 5)  # Use 1/5 of test data
    )
    
    # Create DataLoaders with smaller batch size
    train_loader = DataLoader(
        train_dataset, 
        batch_size=16,  # Smaller batch size
        shuffle=True
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=16, 
        shuffle=False
    )
    
    # Create and train model
    model = FastViT(num_classes=10)
    train_losses, val_accuracies = train_model(model, train_loader, test_loader)
    
    # Plot results
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    
    plt.subplot(1, 2, 2)
    plt.plot(val_accuracies)
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    
    plt.tight_layout()
    plt.show()
    
    # Save the model
    torch.save(model.state_dict(), 'fast_vit_model.pth')

if __name__ == "__main__":
    main()
